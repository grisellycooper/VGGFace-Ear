{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Ear/Face Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "DS_DIR = os.path.join(ROOT_DIR, \"Datasets\", \"VGGFaceEar\", \"ear_augmented\")\n",
    "DS_train_DIR = os.path.join(DS_DIR, \"train\")\n",
    "DS_val_DIR = os.path.join(DS_DIR, \"val\")\n",
    "DS_test_DIR = os.path.join(DS_DIR, \"test\")\n",
    "img_width, img_height = 224, 224\n",
    "batch_size_ = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14868 images belonging to 25 classes.\n",
      "Found 4032 images belonging to 25 classes.\n",
      "Found 192 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "traindata = trdata.flow_from_directory(\n",
    "    directory=DS_train_DIR,\n",
    "    target_size=(img_width,img_height),\n",
    "    batch_size=batch_size_,\n",
    "    shuffle=True,\n",
    "    class_mode='sparse')\n",
    "\n",
    "vldata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "valdata = vldata.flow_from_directory(\n",
    "    directory=DS_val_DIR, \n",
    "    target_size=(img_width,img_height),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    class_mode='sparse')\n",
    "\n",
    "tsdata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "testdata = tsdata.flow_from_directory(\n",
    "    directory=DS_test_DIR, \n",
    "    target_size=(img_width,img_height),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    class_mode='sparse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 25   #clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = VGG16(weights=\"imagenet\", \n",
    "             include_top=False, \n",
    "             input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in cnn.layers:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 25)                102425    \n",
      "=================================================================\n",
      "Total params: 134,362,969\n",
      "Trainable params: 119,648,281\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#1 -> Droput 0.5 LR 0.001\n",
    "#2 -> Dropout 0.3 LR 0.001\n",
    "#3 -> without dropout LR 0.001\n",
    "#4 -> Dropout 0.3 LR 0.0001\n",
    "#5 -> Dropout 0.5 LR 0.0001\n",
    "out = Flatten()(cnn.output)\n",
    "out = (Dense(4096, activation='relu', name='fc1'))(out)\n",
    "out = (Dropout(0.5))(out)\n",
    "out = (Dense(4096, activation='relu', name='fc2'))(out)\n",
    "out = (Dropout(0.5))(out)\n",
    "out = (Dense(set_size, activation='softmax', name='predictions'))(out)\n",
    "\n",
    "model = Model(inputs=[cnn.input], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_face.compile(loss='sparse_categorical_crossentropy',\n",
    "#             optimizer='sgd',\n",
    "#             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(lr=0.1), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])  #vgg16_1.h5--> 58%\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy'])  #vgg16_2.h5--> 99%  #with 2 dropout layers\n",
    "#model_face.compile(optimizer=SGD(lr=0.1, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 99.37%  #without dropout layers\n",
    "#model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 98.75%\n",
    "#model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 55%\n",
    "#model_face.compile(optimizer=RMSprop(lr=0.1), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_3.h5--> 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/465 [..............................] - ETA: 0s - loss: 3.8622 - sparse_categorical_accuracy: 0.0000e+00WARNING:tensorflow:From C:\\Users\\Griss\\anaconda3\\envs\\tf22-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "465/465 [==============================] - ETA: 0s - loss: 3.2673 - sparse_categorical_accuracy: 0.0651\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.14087, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 128ms/step - loss: 3.2673 - sparse_categorical_accuracy: 0.0651 - val_loss: 3.1019 - val_sparse_categorical_accuracy: 0.1409\n",
      "Epoch 2/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 3.0537 - sparse_categorical_accuracy: 0.1174\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.14087 to 0.18180, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 124ms/step - loss: 3.0537 - sparse_categorical_accuracy: 0.1174 - val_loss: 3.0038 - val_sparse_categorical_accuracy: 0.1818\n",
      "Epoch 3/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.9320 - sparse_categorical_accuracy: 0.1597\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.18180 to 0.22669, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 2.9320 - sparse_categorical_accuracy: 0.1597 - val_loss: 2.9137 - val_sparse_categorical_accuracy: 0.2267\n",
      "Epoch 4/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.7986 - sparse_categorical_accuracy: 0.2089\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.22669 to 0.28646, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 2.7986 - sparse_categorical_accuracy: 0.2089 - val_loss: 2.8200 - val_sparse_categorical_accuracy: 0.2865\n",
      "Epoch 5/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.6602 - sparse_categorical_accuracy: 0.2489\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.28646 to 0.31349, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 125ms/step - loss: 2.6602 - sparse_categorical_accuracy: 0.2489 - val_loss: 2.7208 - val_sparse_categorical_accuracy: 0.3135\n",
      "Epoch 6/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.5105 - sparse_categorical_accuracy: 0.3032\n",
      "Epoch 00006: val_sparse_categorical_accuracy did not improve from 0.31349\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 2.5105 - sparse_categorical_accuracy: 0.3032 - val_loss: 2.6216 - val_sparse_categorical_accuracy: 0.3110\n",
      "Epoch 7/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.3708 - sparse_categorical_accuracy: 0.3368\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.31349 to 0.33879, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 60s 128ms/step - loss: 2.3708 - sparse_categorical_accuracy: 0.3368 - val_loss: 2.5061 - val_sparse_categorical_accuracy: 0.3388\n",
      "Epoch 8/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.2384 - sparse_categorical_accuracy: 0.3788\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.33879 to 0.35987, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 60s 130ms/step - loss: 2.2384 - sparse_categorical_accuracy: 0.3788 - val_loss: 2.4072 - val_sparse_categorical_accuracy: 0.3599\n",
      "Epoch 9/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 2.0995 - sparse_categorical_accuracy: 0.4176\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.35987 to 0.38393, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 128ms/step - loss: 2.0995 - sparse_categorical_accuracy: 0.4176 - val_loss: 2.3234 - val_sparse_categorical_accuracy: 0.3839\n",
      "Epoch 10/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.9853 - sparse_categorical_accuracy: 0.4508\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.38393 to 0.40923, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 125ms/step - loss: 1.9853 - sparse_categorical_accuracy: 0.4508 - val_loss: 2.2321 - val_sparse_categorical_accuracy: 0.4092\n",
      "Epoch 11/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.8565 - sparse_categorical_accuracy: 0.4873\n",
      "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.40923\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 1.8565 - sparse_categorical_accuracy: 0.4873 - val_loss: 2.2009 - val_sparse_categorical_accuracy: 0.4043\n",
      "Epoch 12/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.7428 - sparse_categorical_accuracy: 0.5186- ETA: 3s - loss: 1.7476 - s\n",
      "Epoch 00012: val_sparse_categorical_accuracy improved from 0.40923 to 0.43403, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 126ms/step - loss: 1.7428 - sparse_categorical_accuracy: 0.5186 - val_loss: 2.1268 - val_sparse_categorical_accuracy: 0.4340\n",
      "Epoch 13/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.6396 - sparse_categorical_accuracy: 0.5462\n",
      "Epoch 00013: val_sparse_categorical_accuracy improved from 0.43403 to 0.45263, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 62s 133ms/step - loss: 1.6396 - sparse_categorical_accuracy: 0.5462 - val_loss: 2.0595 - val_sparse_categorical_accuracy: 0.4526\n",
      "Epoch 14/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.5432 - sparse_categorical_accuracy: 0.5779\n",
      "Epoch 00014: val_sparse_categorical_accuracy improved from 0.45263 to 0.45957, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 1.5432 - sparse_categorical_accuracy: 0.5779 - val_loss: 2.0212 - val_sparse_categorical_accuracy: 0.4596\n",
      "Epoch 15/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.4496 - sparse_categorical_accuracy: 0.6035\n",
      "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.45957\n",
      "465/465 [==============================] - 56s 121ms/step - loss: 1.4496 - sparse_categorical_accuracy: 0.6035 - val_loss: 2.0016 - val_sparse_categorical_accuracy: 0.4410\n",
      "Epoch 16/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.3729 - sparse_categorical_accuracy: 0.6229\n",
      "Epoch 00016: val_sparse_categorical_accuracy improved from 0.45957 to 0.47321, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 1.3729 - sparse_categorical_accuracy: 0.6229 - val_loss: 1.9481 - val_sparse_categorical_accuracy: 0.4732\n",
      "Epoch 17/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.2943 - sparse_categorical_accuracy: 0.6456\n",
      "Epoch 00017: val_sparse_categorical_accuracy did not improve from 0.47321\n",
      "465/465 [==============================] - 56s 119ms/step - loss: 1.2943 - sparse_categorical_accuracy: 0.6456 - val_loss: 1.9285 - val_sparse_categorical_accuracy: 0.4690\n",
      "Epoch 18/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.2145 - sparse_categorical_accuracy: 0.6708\n",
      "Epoch 00018: val_sparse_categorical_accuracy improved from 0.47321 to 0.49033, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 125ms/step - loss: 1.2145 - sparse_categorical_accuracy: 0.6708 - val_loss: 1.8750 - val_sparse_categorical_accuracy: 0.4903\n",
      "Epoch 19/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.1529 - sparse_categorical_accuracy: 0.6863\n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.49033\n",
      "465/465 [==============================] - 56s 120ms/step - loss: 1.1529 - sparse_categorical_accuracy: 0.6863 - val_loss: 1.8638 - val_sparse_categorical_accuracy: 0.4844\n",
      "Epoch 20/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.0962 - sparse_categorical_accuracy: 0.7012\n",
      "Epoch 00020: val_sparse_categorical_accuracy improved from 0.49033 to 0.49702, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 63s 136ms/step - loss: 1.0962 - sparse_categorical_accuracy: 0.7012 - val_loss: 1.8343 - val_sparse_categorical_accuracy: 0.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 1.0413 - sparse_categorical_accuracy: 0.7203\n",
      "Epoch 00021: val_sparse_categorical_accuracy did not improve from 0.49702\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 1.0413 - sparse_categorical_accuracy: 0.7203 - val_loss: 1.8463 - val_sparse_categorical_accuracy: 0.4888\n",
      "Epoch 22/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.9853 - sparse_categorical_accuracy: 0.7360\n",
      "Epoch 00022: val_sparse_categorical_accuracy improved from 0.49702 to 0.49975, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 126ms/step - loss: 0.9853 - sparse_categorical_accuracy: 0.7360 - val_loss: 1.7930 - val_sparse_categorical_accuracy: 0.4998\n",
      "Epoch 23/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.9336 - sparse_categorical_accuracy: 0.7485\n",
      "Epoch 00023: val_sparse_categorical_accuracy improved from 0.49975 to 0.50670, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 127ms/step - loss: 0.9336 - sparse_categorical_accuracy: 0.7485 - val_loss: 1.7873 - val_sparse_categorical_accuracy: 0.5067\n",
      "Epoch 24/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.8818 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 00024: val_sparse_categorical_accuracy improved from 0.50670 to 0.50868, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.8818 - sparse_categorical_accuracy: 0.7622 - val_loss: 1.7572 - val_sparse_categorical_accuracy: 0.5087\n",
      "Epoch 25/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.8439 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 00025: val_sparse_categorical_accuracy improved from 0.50868 to 0.50918, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 126ms/step - loss: 0.8439 - sparse_categorical_accuracy: 0.7675 - val_loss: 1.8036 - val_sparse_categorical_accuracy: 0.5092\n",
      "Epoch 26/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.8083 - sparse_categorical_accuracy: 0.7820\n",
      "Epoch 00026: val_sparse_categorical_accuracy improved from 0.50918 to 0.51314, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 125ms/step - loss: 0.8083 - sparse_categorical_accuracy: 0.7820 - val_loss: 1.7561 - val_sparse_categorical_accuracy: 0.5131\n",
      "Epoch 27/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.7667 - sparse_categorical_accuracy: 0.7922\n",
      "Epoch 00027: val_sparse_categorical_accuracy did not improve from 0.51314\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.7667 - sparse_categorical_accuracy: 0.7922 - val_loss: 1.7540 - val_sparse_categorical_accuracy: 0.5097\n",
      "Epoch 28/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.7292 - sparse_categorical_accuracy: 0.8086\n",
      "Epoch 00028: val_sparse_categorical_accuracy improved from 0.51314 to 0.52307, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 126ms/step - loss: 0.7292 - sparse_categorical_accuracy: 0.8086 - val_loss: 1.7291 - val_sparse_categorical_accuracy: 0.5231\n",
      "Epoch 29/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.6960 - sparse_categorical_accuracy: 0.8151\n",
      "Epoch 00029: val_sparse_categorical_accuracy improved from 0.52307 to 0.52753, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 126ms/step - loss: 0.6960 - sparse_categorical_accuracy: 0.8151 - val_loss: 1.7317 - val_sparse_categorical_accuracy: 0.5275\n",
      "Epoch 30/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.6676 - sparse_categorical_accuracy: 0.8241\n",
      "Epoch 00030: val_sparse_categorical_accuracy did not improve from 0.52753\n",
      "465/465 [==============================] - 56s 120ms/step - loss: 0.6676 - sparse_categorical_accuracy: 0.8241 - val_loss: 1.7171 - val_sparse_categorical_accuracy: 0.5243\n",
      "Epoch 31/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.6415 - sparse_categorical_accuracy: 0.8307\n",
      "Epoch 00031: val_sparse_categorical_accuracy did not improve from 0.52753\n",
      "465/465 [==============================] - 56s 120ms/step - loss: 0.6415 - sparse_categorical_accuracy: 0.8307 - val_loss: 1.7208 - val_sparse_categorical_accuracy: 0.5268\n",
      "Epoch 32/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.6156 - sparse_categorical_accuracy: 0.8392\n",
      "Epoch 00032: val_sparse_categorical_accuracy improved from 0.52753 to 0.53844, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.6156 - sparse_categorical_accuracy: 0.8392 - val_loss: 1.6808 - val_sparse_categorical_accuracy: 0.5384\n",
      "Epoch 33/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.5910 - sparse_categorical_accuracy: 0.8413\n",
      "Epoch 00033: val_sparse_categorical_accuracy did not improve from 0.53844\n",
      "465/465 [==============================] - 56s 120ms/step - loss: 0.5910 - sparse_categorical_accuracy: 0.8413 - val_loss: 1.7202 - val_sparse_categorical_accuracy: 0.5332\n",
      "Epoch 34/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.5706 - sparse_categorical_accuracy: 0.8491\n",
      "Epoch 00034: val_sparse_categorical_accuracy improved from 0.53844 to 0.53869, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 127ms/step - loss: 0.5706 - sparse_categorical_accuracy: 0.8491 - val_loss: 1.6820 - val_sparse_categorical_accuracy: 0.5387\n",
      "Epoch 35/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.5406 - sparse_categorical_accuracy: 0.8604\n",
      "Epoch 00035: val_sparse_categorical_accuracy did not improve from 0.53869\n",
      "465/465 [==============================] - 56s 119ms/step - loss: 0.5406 - sparse_categorical_accuracy: 0.8604 - val_loss: 1.6816 - val_sparse_categorical_accuracy: 0.5283\n",
      "Epoch 36/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.5247 - sparse_categorical_accuracy: 0.8633\n",
      "Epoch 00036: val_sparse_categorical_accuracy improved from 0.53869 to 0.53894, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.5247 - sparse_categorical_accuracy: 0.8633 - val_loss: 1.7190 - val_sparse_categorical_accuracy: 0.5389\n",
      "Epoch 37/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.5049 - sparse_categorical_accuracy: 0.8695\n",
      "Epoch 00037: val_sparse_categorical_accuracy did not improve from 0.53894\n",
      "465/465 [==============================] - 56s 119ms/step - loss: 0.5049 - sparse_categorical_accuracy: 0.8695 - val_loss: 1.7591 - val_sparse_categorical_accuracy: 0.5285\n",
      "Epoch 38/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4837 - sparse_categorical_accuracy: 0.8755\n",
      "Epoch 00038: val_sparse_categorical_accuracy did not improve from 0.53894\n",
      "465/465 [==============================] - 56s 119ms/step - loss: 0.4837 - sparse_categorical_accuracy: 0.8755 - val_loss: 1.7002 - val_sparse_categorical_accuracy: 0.5360\n",
      "Epoch 39/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4680 - sparse_categorical_accuracy: 0.8778\n",
      "Epoch 00039: val_sparse_categorical_accuracy improved from 0.53894 to 0.54315, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.4680 - sparse_categorical_accuracy: 0.8778 - val_loss: 1.7191 - val_sparse_categorical_accuracy: 0.5432\n",
      "Epoch 40/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4471 - sparse_categorical_accuracy: 0.8842\n",
      "Epoch 00040: val_sparse_categorical_accuracy improved from 0.54315 to 0.54638, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.4471 - sparse_categorical_accuracy: 0.8842 - val_loss: 1.7115 - val_sparse_categorical_accuracy: 0.5464\n",
      "Epoch 41/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4359 - sparse_categorical_accuracy: 0.8870\n",
      "Epoch 00041: val_sparse_categorical_accuracy did not improve from 0.54638\n",
      "465/465 [==============================] - 56s 120ms/step - loss: 0.4359 - sparse_categorical_accuracy: 0.8870 - val_loss: 1.7679 - val_sparse_categorical_accuracy: 0.5310\n",
      "Epoch 42/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4275 - sparse_categorical_accuracy: 0.8897\n",
      "Epoch 00042: val_sparse_categorical_accuracy did not improve from 0.54638\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.4275 - sparse_categorical_accuracy: 0.8897 - val_loss: 1.6957 - val_sparse_categorical_accuracy: 0.5429\n",
      "Epoch 43/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.4041 - sparse_categorical_accuracy: 0.8990\n",
      "Epoch 00043: val_sparse_categorical_accuracy did not improve from 0.54638\n",
      "465/465 [==============================] - 56s 119ms/step - loss: 0.4041 - sparse_categorical_accuracy: 0.8990 - val_loss: 1.6794 - val_sparse_categorical_accuracy: 0.5417\n",
      "Epoch 44/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3995 - sparse_categorical_accuracy: 0.8967\n",
      "Epoch 00044: val_sparse_categorical_accuracy did not improve from 0.54638\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.3995 - sparse_categorical_accuracy: 0.8967 - val_loss: 1.7750 - val_sparse_categorical_accuracy: 0.5273\n",
      "Epoch 45/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3865 - sparse_categorical_accuracy: 0.9002\n",
      "Epoch 00045: val_sparse_categorical_accuracy improved from 0.54638 to 0.55754, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 58s 125ms/step - loss: 0.3865 - sparse_categorical_accuracy: 0.9002 - val_loss: 1.6920 - val_sparse_categorical_accuracy: 0.5575\n",
      "Epoch 46/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3753 - sparse_categorical_accuracy: 0.9041\n",
      "Epoch 00046: val_sparse_categorical_accuracy did not improve from 0.55754\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.9041 - val_loss: 1.7420 - val_sparse_categorical_accuracy: 0.5422\n",
      "Epoch 47/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3634 - sparse_categorical_accuracy: 0.9095\n",
      "Epoch 00047: val_sparse_categorical_accuracy improved from 0.55754 to 0.55804, saving model to vgg16_Ear_aug.h5\n",
      "465/465 [==============================] - 59s 126ms/step - loss: 0.3634 - sparse_categorical_accuracy: 0.9095 - val_loss: 1.6842 - val_sparse_categorical_accuracy: 0.5580\n",
      "Epoch 48/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3486 - sparse_categorical_accuracy: 0.9158\n",
      "Epoch 00048: val_sparse_categorical_accuracy did not improve from 0.55804\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.3486 - sparse_categorical_accuracy: 0.9158 - val_loss: 1.7177 - val_sparse_categorical_accuracy: 0.5506\n",
      "Epoch 49/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3403 - sparse_categorical_accuracy: 0.9144\n",
      "Epoch 00049: val_sparse_categorical_accuracy did not improve from 0.55804\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.3403 - sparse_categorical_accuracy: 0.9144 - val_loss: 1.7032 - val_sparse_categorical_accuracy: 0.5565\n",
      "Epoch 50/50\n",
      "465/465 [==============================] - ETA: 0s - loss: 0.3292 - sparse_categorical_accuracy: 0.9173\n",
      "Epoch 00050: val_sparse_categorical_accuracy did not improve from 0.55804\n",
      "465/465 [==============================] - 55s 119ms/step - loss: 0.3292 - sparse_categorical_accuracy: 0.9173 - val_loss: 1.6900 - val_sparse_categorical_accuracy: 0.5538\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "tensorboardcb = keras.callbacks.TensorBoard(log_dir='Tensorboard_logs', histogram_freq=0, write_graph=True)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"vgg16_Ear_aug.h5\", \n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    mode='auto')\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    min_delta=0, \n",
    "    patience=15,\n",
    "    verbose=1, \n",
    "    mode='auto')\n",
    "\n",
    "hist = model.fit(\n",
    "    x=traindata, \n",
    "    #steps_per_epoch=traindata.samples//batch_size_,\n",
    "    validation_data= valdata, \n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint,early,tensorboardcb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating before Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 51s 403ms/step - loss: 1.2370 - sparse_categorical_accuracy: 0.7121\n",
      "test loss, test acc: [1.2369836568832397, 0.7120535969734192]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(valdata)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 26s 138ms/step - loss: 1.2656 - sparse_categorical_accuracy: 0.7344\n",
      "test loss, test acc: [1.2655636072158813, 0.734375]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(testdata)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 11s 84ms/step - loss: 1.6900 - sparse_categorical_accuracy: 0.5538\n",
      "test loss, test acc: [1.6900315284729004, 0.5538194179534912]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(valdata)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 2s 9ms/step - loss: 1.6553 - sparse_categorical_accuracy: 0.6094\n",
      "test loss, test acc: [1.6553372144699097, 0.609375]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(testdata)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuse training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "DS_DIR_EAR = os.path.join(ROOT_DIR, \"Datasets\", \"VGGFaceEar\", \"ear_25class\")\n",
    "DS_DIR_FACE = os.path.join(ROOT_DIR, \"Datasets\", \"VGGFaceEar\", \"face_25class\")\n",
    "img_width, img_height = 224, 224\n",
    "batch_size_ = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,dir1, dir2, batch_size,img_height,img_width):\n",
    "    genX1 = generator.flow_from_directory(dir1,\n",
    "                              target_size = (img_height,img_width),\n",
    "                              class_mode = 'sparse',\n",
    "                              batch_size = batch_size,\n",
    "                              shuffle=False)\n",
    "    \n",
    "    genX2 = generator.flow_from_directory(dir2,\n",
    "                              target_size = (img_height, img_width),\n",
    "                              class_mode = 'sparse',\n",
    "                              batch_size = batch_size, \n",
    "                              shuffle=False)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        yield [X1i[0], X2i[0]], X2i[1]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "traindata_face_ear = generate_generator_multiple(\n",
    "    generator = trdata,\n",
    "    dir1 = os.path.join(DS_DIR_FACE, \"train\"),\n",
    "    dir2 = os.path.join(DS_DIR_EAR, \"train\"),\n",
    "    batch_size = batch_size_,\n",
    "    img_height = img_height,\n",
    "    img_width = img_width,\n",
    ")\n",
    "\n",
    "vldata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "valdata_face_ear = generate_generator_multiple(\n",
    "    generator = vldata,\n",
    "    dir1 = os.path.join(DS_DIR_FACE, \"val\"),\n",
    "    dir2 = os.path.join(DS_DIR_EAR, \"val\"),\n",
    "    batch_size = batch_size_,\n",
    "    img_height = img_height,\n",
    "    img_width = img_width,\n",
    ")\n",
    "\n",
    "tsdata = ImageDataGenerator(\n",
    "    rescale=1./255.)\n",
    "testdata_face_ear = generate_generator_multiple(\n",
    "    generator = tsdata,\n",
    "    dir1 = os.path.join(DS_DIR_FACE, \"test\"),\n",
    "    dir2 = os.path.join(DS_DIR_EAR, \"test\"),\n",
    "    batch_size = batch_size_,\n",
    "    img_height = img_height,\n",
    "    img_width = img_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 708 images belonging to 25 classes.\n",
      "Found 708 images belonging to 25 classes.\n",
      "Found 192 images belonging to 25 classes.\n",
      "Found 192 images belonging to 25 classes.\n",
      "Found 192 images belonging to 25 classes.\n",
      "Found 192 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "a = next(traindata_face_ear)\n",
    "b = next(valdata_face_ear)\n",
    "c = next(testdata_face_ear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 25   #clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_face = load_model(\"TrainedModels/vgg16_Face_aug.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_face.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = layer._name + \"_cnn_face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = model_face.layers[-4].output\n",
    "#out1 = (BatchNormalization()(out1))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ear = load_model(\"TrainedModels/vgg16_Ear_aug.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_ear.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = layer._name + \"_cnn_ear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = model_ear.layers[-4].output\n",
    "#out2 = (BatchNormalization()(out2))\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Concatenate()([out1, out2])\n",
    "\n",
    "#out = Flatten()(con)\n",
    "#out = (Dense(4096, activation='relu', name='fc1'))(con)\n",
    "#out = (Dropout(0.3))(out)\n",
    "out = (Dense(4096, activation='relu', name='fc2'))(con)\n",
    "out = (Dropout(0.5))(out)\n",
    "out = (Dense(set_size, activation='softmax', name='predictions'))(out)\n",
    "\n",
    "model = Model(inputs=[model_face.input, model_ear.input], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1_cnn_face (InputLayer)   [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1_cnn_ear (InputLayer)    [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_cnn_face (Conv2D)  (None, 224, 224, 64) 1792        input_1_cnn_face[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_cnn_ear (Conv2D)   (None, 224, 224, 64) 1792        input_1_cnn_ear[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_cnn_face (Conv2D)  (None, 224, 224, 64) 36928       block1_conv1_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_cnn_ear (Conv2D)   (None, 224, 224, 64) 36928       block1_conv1_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool_cnn_face (MaxPoolin (None, 112, 112, 64) 0           block1_conv2_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool_cnn_ear (MaxPooling (None, 112, 112, 64) 0           block1_conv2_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1_cnn_face (Conv2D)  (None, 112, 112, 128 73856       block1_pool_cnn_face[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1_cnn_ear (Conv2D)   (None, 112, 112, 128 73856       block1_pool_cnn_ear[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2_cnn_face (Conv2D)  (None, 112, 112, 128 147584      block2_conv1_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2_cnn_ear (Conv2D)   (None, 112, 112, 128 147584      block2_conv1_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool_cnn_face (MaxPoolin (None, 56, 56, 128)  0           block2_conv2_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool_cnn_ear (MaxPooling (None, 56, 56, 128)  0           block2_conv2_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1_cnn_face (Conv2D)  (None, 56, 56, 256)  295168      block2_pool_cnn_face[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1_cnn_ear (Conv2D)   (None, 56, 56, 256)  295168      block2_pool_cnn_ear[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2_cnn_face (Conv2D)  (None, 56, 56, 256)  590080      block3_conv1_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2_cnn_ear (Conv2D)   (None, 56, 56, 256)  590080      block3_conv1_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3_cnn_face (Conv2D)  (None, 56, 56, 256)  590080      block3_conv2_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3_cnn_ear (Conv2D)   (None, 56, 56, 256)  590080      block3_conv2_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool_cnn_face (MaxPoolin (None, 28, 28, 256)  0           block3_conv3_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool_cnn_ear (MaxPooling (None, 28, 28, 256)  0           block3_conv3_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1_cnn_face (Conv2D)  (None, 28, 28, 512)  1180160     block3_pool_cnn_face[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1_cnn_ear (Conv2D)   (None, 28, 28, 512)  1180160     block3_pool_cnn_ear[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2_cnn_face (Conv2D)  (None, 28, 28, 512)  2359808     block4_conv1_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2_cnn_ear (Conv2D)   (None, 28, 28, 512)  2359808     block4_conv1_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3_cnn_face (Conv2D)  (None, 28, 28, 512)  2359808     block4_conv2_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3_cnn_ear (Conv2D)   (None, 28, 28, 512)  2359808     block4_conv2_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool_cnn_face (MaxPoolin (None, 14, 14, 512)  0           block4_conv3_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool_cnn_ear (MaxPooling (None, 14, 14, 512)  0           block4_conv3_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1_cnn_face (Conv2D)  (None, 14, 14, 512)  2359808     block4_pool_cnn_face[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1_cnn_ear (Conv2D)   (None, 14, 14, 512)  2359808     block4_pool_cnn_ear[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2_cnn_face (Conv2D)  (None, 14, 14, 512)  2359808     block5_conv1_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2_cnn_ear (Conv2D)   (None, 14, 14, 512)  2359808     block5_conv1_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3_cnn_face (Conv2D)  (None, 14, 14, 512)  2359808     block5_conv2_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3_cnn_ear (Conv2D)   (None, 14, 14, 512)  2359808     block5_conv2_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool_cnn_face (MaxPoolin (None, 7, 7, 512)    0           block5_conv3_cnn_face[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool_cnn_ear (MaxPooling (None, 7, 7, 512)    0           block5_conv3_cnn_ear[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1_cnn_face (Flatten)    (None, 25088)        0           block5_pool_cnn_face[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_cnn_ear (Flatten)       (None, 25088)        0           block5_pool_cnn_ear[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "fc1_cnn_face (Dense)            (None, 4096)         102764544   flatten_1_cnn_face[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "fc1_cnn_ear (Dense)             (None, 4096)         102764544   flatten_cnn_ear[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_cnn_face (Dropout)      (None, 4096)         0           fc1_cnn_face[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_cnn_ear (Dropout)       (None, 4096)         0           fc1_cnn_ear[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8192)         0           dropout_cnn_face[0][0]           \n",
      "                                                                 dropout_cnn_ear[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 4096)         33558528    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           fc2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 25)           102425      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 268,619,417\n",
      "Trainable params: 33,660,953\n",
      "Non-trainable params: 234,958,464\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(lr=0.1), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_1.h5--> 58%\n",
    "#model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy'])  #vgg16_2.h5--> 99%  #with 2 dropout layers\n",
    "model.compile(optimizer=SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True), loss=keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy'])  #vgg16_2.h5--> 99%  #with 2 dropout layers\n",
    "\n",
    "#model.compile(optimizer=SGD(lr=0.1, momentum=0.9), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 99.37%  #without dropout layers\n",
    "#model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 98.75%\n",
    "#model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])  #vgg16_2.h5--> 55%\n",
    "#model.compile(optimizer=RMSprop(lr=0.1), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])  #vgg16_3.h5--> 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([traindata_face, traindata_ear], [valdata_face, valdata_ear], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 3.1671 - sparse_categorical_accuracy: 0.1250WARNING:tensorflow:From C:\\Users\\Griss\\anaconda3\\envs\\tf22-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "22/22 [==============================] - ETA: 0s - loss: 4.4963 - sparse_categorical_accuracy: 0.0074\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.18229, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 9s 411ms/step - loss: 4.4963 - sparse_categorical_accuracy: 0.0074 - val_loss: 3.1295 - val_sparse_categorical_accuracy: 0.1823\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 3.7855 - sparse_categorical_accuracy: 0.0170\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.18229 to 0.51562, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 382ms/step - loss: 3.7855 - sparse_categorical_accuracy: 0.0170 - val_loss: 2.3473 - val_sparse_categorical_accuracy: 0.5156\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.4166 - sparse_categorical_accuracy: 0.3595\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.51562 to 0.71875, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 27s 1s/step - loss: 2.4166 - sparse_categorical_accuracy: 0.3595 - val_loss: 1.9588 - val_sparse_categorical_accuracy: 0.7188\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.9174 - sparse_categorical_accuracy: 0.5266\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.71875 to 0.72917, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 377ms/step - loss: 1.9174 - sparse_categorical_accuracy: 0.5266 - val_loss: 1.6219 - val_sparse_categorical_accuracy: 0.7292\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4740 - sparse_categorical_accuracy: 0.7086\n",
      "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.72917\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.4740 - sparse_categorical_accuracy: 0.7086 - val_loss: 1.3548 - val_sparse_categorical_accuracy: 0.7135\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0524 - sparse_categorical_accuracy: 0.8299\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.72917 to 0.73958, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 376ms/step - loss: 1.0524 - sparse_categorical_accuracy: 0.8299 - val_loss: 1.1612 - val_sparse_categorical_accuracy: 0.7396\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8199 - sparse_categorical_accuracy: 0.8743\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.73958 to 0.78125, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 376ms/step - loss: 0.8199 - sparse_categorical_accuracy: 0.8743 - val_loss: 1.0269 - val_sparse_categorical_accuracy: 0.7812\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5689 - sparse_categorical_accuracy: 0.9497\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.78125 to 0.79167, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.5689 - sparse_categorical_accuracy: 0.9497 - val_loss: 0.9273 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4142 - sparse_categorical_accuracy: 0.9660\n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.79167\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.4142 - sparse_categorical_accuracy: 0.9660 - val_loss: 0.8778 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3121 - sparse_categorical_accuracy: 0.9749\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.79167 to 0.80729, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 21s 934ms/step - loss: 0.3121 - sparse_categorical_accuracy: 0.9749 - val_loss: 0.8205 - val_sparse_categorical_accuracy: 0.8073\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2660 - sparse_categorical_accuracy: 0.9852\n",
      "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.2660 - sparse_categorical_accuracy: 0.9852 - val_loss: 0.7926 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2351 - sparse_categorical_accuracy: 0.9822\n",
      "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 0.2351 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.7776 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1916 - sparse_categorical_accuracy: 0.9926\n",
      "Epoch 00013: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 211ms/step - loss: 0.1916 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.7596 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1673 - sparse_categorical_accuracy: 0.9926\n",
      "Epoch 00014: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 213ms/step - loss: 0.1673 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.7418 - val_sparse_categorical_accuracy: 0.7917\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1401 - sparse_categorical_accuracy: 0.9911\n",
      "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 0.1401 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.7326 - val_sparse_categorical_accuracy: 0.8021\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1348 - sparse_categorical_accuracy: 0.9956\n",
      "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.80729\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 0.1348 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.7344 - val_sparse_categorical_accuracy: 0.7969\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1338 - sparse_categorical_accuracy: 0.9941\n",
      "Epoch 00017: val_sparse_categorical_accuracy improved from 0.80729 to 0.81771, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 383ms/step - loss: 0.1338 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.7054 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1184 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00018: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.1184 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6974 - val_sparse_categorical_accuracy: 0.8073\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1003 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6909 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0961 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.0961 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6959 - val_sparse_categorical_accuracy: 0.8021\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0889 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00021: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0889 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6854 - val_sparse_categorical_accuracy: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0748 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00022: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6762 - val_sparse_categorical_accuracy: 0.8073\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0821 - sparse_categorical_accuracy: 0.9956\n",
      "Epoch 00023: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.6840 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 24/50\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0798 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00024: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0795 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6665 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0769 - sparse_categorical_accuracy: 0.9972\n",
      "Epoch 00025: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.0769 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.6639 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0713 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00026: val_sparse_categorical_accuracy did not improve from 0.81771\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.0713 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6554 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0632 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00027: val_sparse_categorical_accuracy improved from 0.81771 to 0.82292, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 383ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6514 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0595 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00028: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.0595 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6600 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0613 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00029: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6575 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0592 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00030: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0592 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6544 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0495 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00031: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0495 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6585 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0544 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00032: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.0544 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6488 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0475 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00033: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.0475 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6481 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0455 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00034: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.0455 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6510 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0444 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00035: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0444 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6421 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0463 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00036: val_sparse_categorical_accuracy did not improve from 0.82292\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6391 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0466 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00037: val_sparse_categorical_accuracy improved from 0.82292 to 0.82812, saving model to vgg16_Face_Ear.h5\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.0466 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6351 - val_sparse_categorical_accuracy: 0.8281\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0410 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00038: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.0410 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6390 - val_sparse_categorical_accuracy: 0.8281\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0450 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00039: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6402 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0420 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00040: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.0420 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6372 - val_sparse_categorical_accuracy: 0.8073\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0404 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00041: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6457 - val_sparse_categorical_accuracy: 0.8021\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0428 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00042: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6322 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0389 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00043: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.0389 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6304 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0379 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00044: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0379 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6337 - val_sparse_categorical_accuracy: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0352 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00045: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.0352 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6250 - val_sparse_categorical_accuracy: 0.8281\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0335 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 00046: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6194 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 47/50\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0335 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00047: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0334 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6222 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0303 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00048: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.0303 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6271 - val_sparse_categorical_accuracy: 0.8177\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0345 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 00049: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6158 - val_sparse_categorical_accuracy: 0.8281\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0330 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00050: val_sparse_categorical_accuracy did not improve from 0.82812\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.0330 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6296 - val_sparse_categorical_accuracy: 0.8229\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "tensorboardcb = keras.callbacks.TensorBoard(log_dir='Tensorboard_logs/multi/', histogram_freq=0, write_graph=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"vgg16_Face_Ear.h5\", \n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    mode='auto')\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    min_delta=0, \n",
    "    patience=15,\n",
    "    #if it doesnt see any rise in validation accuracy in 25,  the model will stop \n",
    "    verbose=1, \n",
    "    mode='auto')\n",
    "\n",
    "hist = model.fit(\n",
    "    traindata_face_ear,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=708//32,\n",
    "    validation_data=valdata_face_ear,\n",
    "    validation_steps = 192//32,\n",
    "    callbacks=[checkpoint,early,tensorboardcb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 34s 179ms/step - loss: 0.6296 - sparse_categorical_accuracy: 0.8229\n",
      "test loss, test acc: [0.6295827031135559, 0.8229166865348816]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(valdata_face_ear, steps= 192)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 34s 179ms/step - loss: 0.6484 - sparse_categorical_accuracy: 0.8021\n",
      "test loss, test acc: [0.6484200358390808, 0.8020833134651184]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(testdata_face_ear, steps= 192)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
